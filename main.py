import torch
import google.generativeai as genai
from faster_whisper import WhisperModel
from pyannote.audio.pipelines.speaker_diarization import PretrainedSpeakerDiarizationPipeline
from pyannote.core import Segment
import os
import datetime
import re

# ğŸ”¹ [ì „ì—­ ë³€ìˆ˜] ì„¤ì • ê°’ ëª¨ì•„ë‘ê¸°
class Config:
    GEMINI_API_KEY = "YOUR_GEMINI_API_KEY"  # ğŸ”‘ Gemini API í‚¤ (ë³€ê²½ í•„ìš”)
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"  # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    WHISPER_MODEL_SIZE = "medium"  # Whisper ëª¨ë¸ í¬ê¸° ì„ íƒ (small, medium, large ê°€ëŠ¥)
    SUPPORTED_AUDIO_EXTENSIONS = ('.mp3', '.wav', '.m4a', '.flac')  # ì§€ì›í•˜ëŠ” ìŒì„± íŒŒì¼ í™•ì¥ì
    
    # ğŸ”¹ [ì „ì—­ ë³€ìˆ˜] LLM í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    SYSTEM_PROMPT = """
    ë‹¹ì‹ ì€ íšŒì˜ë¡ ì‘ì„± ë° ë¶„ì„ì— íŠ¹í™”ëœ ì „ë¬¸ ë¹„ì„œì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ íšŒì˜ë¡ ì›ë¬¸ê³¼ í™”ì ì •ë³´ë¥¼ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ì—¬, 
    íšŒì˜ ìš”ì•½, ì£¼ìš” ê²°ì • ì‚¬í•­, ê·¸ë¦¬ê³  ì ì ˆí•œ íƒœê·¸ë¥¼ ì •í™•í•˜ê²Œ ìƒì„±í•˜ëŠ” ë° ë›°ì–´ë‚œ ì—­ëŸ‰ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
    íšŒì˜ë¡ì˜ ì¤‘ìš”ì„±ì„ ì¸ì§€í•˜ê³ , í•µì‹¬ ë‚´ìš©ë§Œì„ íŒŒì•…í•˜ëŠ” ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
    """

    USER_PROMPT_TEMPLATE = """
    ì£¼ì–´ì§„ íšŒì˜ë¡ ì›ë¬¸ê³¼ ê° ë°œì–¸ì˜ í™”ì ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì— ë§ì¶° íšŒì˜ë¡ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    
    <ìš”êµ¬ ì‚¬í•­>
    1.  íšŒì˜ ìš”ì•½: íšŒì˜ì—ì„œ ë…¼ì˜ëœ ì£¼ìš” ë‚´ìš©ì„ 3~5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.
    2.  ì£¼ìš” ê²°ì • ì‚¬í•­: íšŒì˜ì—ì„œ ê²°ì •ëœ ì‚¬í•­ì„ ëª…í™•í•˜ê²Œ ëª©ë¡ í˜•íƒœë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
        - ê° ê²°ì • ì‚¬í•­ì€ ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.
    3.  íƒœê·¸: íšŒì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ íƒœê·¸ë“¤ì„ í•´ì‹œíƒœê·¸(#) í˜•ì‹ìœ¼ë¡œ ë‚˜ì—´í•©ë‹ˆë‹¤. (#í”„ë¡œì íŠ¸, #ì˜ì‚¬ê²°ì •, #TODO, #ê²°ì •ì‚¬í•­, #ê¸´ê¸‰, #ì°¸ê³ ì‚¬í•­ ë“±). íƒœê·¸ëŠ” 3ê°œ ì´ìƒ ì‘ì„±í•©ë‹ˆë‹¤.
    
    <ì¶œë ¥ í˜•ì‹>
    ## íšŒì˜ ìš”ì•½
    - [ìš”ì•½ëœ ë‚´ìš© 1]
    - [ìš”ì•½ëœ ë‚´ìš© 2]
    - ...

    ## ì£¼ìš” ê²°ì • ì‚¬í•­
    - [ê²°ì • ì‚¬í•­ 1]
    - [ê²°ì • ì‚¬í•­ 2]
    - ...

    ## íƒœê·¸
    - #íƒœê·¸1
    - #íƒœê·¸2
    - ...

    <ì˜ˆì‹œ>
    ## íšŒì˜ ìš”ì•½
    - ì´ë²ˆ íšŒì˜ì—ì„œëŠ” ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ ê¸°íšì— ëŒ€í•œ ë…¼ì˜ê°€ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.
    - ë§ˆì¼€íŒ… ì „ëµì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë°©í–¥ê³¼ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ì˜€ìŠµë‹ˆë‹¤.
    - ë‹¤ìŒ íšŒì˜ì—ì„œëŠ” ê° íŒ€ì˜ ì—­í• ì„ ë¶„ë‹´í•˜ê³ , ì„¸ë¶€ ì¶”ì§„ ê³„íšì„ ê²€í† í•˜ê¸°ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤.

    ## ì£¼ìš” ê²°ì • ì‚¬í•­
    - ì‹ ê·œ í”„ë¡œì íŠ¸ 'Alpha'ì˜ ì‹œì‘ì„ ìŠ¹ì¸í•©ë‹ˆë‹¤.
    - ë§ˆì¼€íŒ… íŒ€ì€ ë‹¤ìŒ ì£¼ê¹Œì§€ ë§ˆì¼€íŒ… ì „ëµì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê³„íšì„ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤.

    ## íƒœê·¸
    - #í”„ë¡œì íŠ¸ê¸°íš #ë§ˆì¼€íŒ…ì „ëµ #ì˜ì‚¬ê²°ì • #Alpha #TODO #ê¸´ê¸‰
    
    <íšŒì˜ë¡ ì›ë¬¸>
    {speaker_transcript}
    """

    # ğŸ”¹ LLM(Gemini) íŒŒë¼ë¯¸í„° ì„¤ì • (ìƒˆë¡œ ì¶”ê°€ë¨)
    TEMPERATURE = 0.4  # 0.0 ~ 1.0 ì‚¬ì´ì˜ ê°’, ë‚®ì„ìˆ˜ë¡ ê²°ì •ë¡ ì , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì  (ê¸°ë³¸ê°’: 0.9)
    TOP_P = 0.95  # ëˆ„ì  í™•ë¥  ê°’, ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ê²°ê³¼ ìƒì„± (ê¸°ë³¸ê°’: 1.0)
    TOP_K = 40  # í™•ë¥ ì´ ë†’ì€ í† í° ìƒìœ„ Kê°œë§Œ ê³ ë ¤ (ê¸°ë³¸ê°’: 40)
    MAX_OUTPUT_TOKENS = 2048  # ìµœëŒ€ ì¶œë ¥ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 2048)
    
# ì„¤ì • ê°ì²´ ìƒì„±
config = Config()

# ğŸ”¹ Whisper ëª¨ë¸ ë¡œë“œ
whisper_model = WhisperModel(config.WHISPER_MODEL_SIZE, device=config.DEVICE, compute_type="float16")

# ğŸ”¹ Pyannote í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ
diarization_pipeline = PretrainedSpeakerDiarizationPipeline.from_pretrained("pyannote/speaker-diarization")
diarization_pipeline.to(config.DEVICE)

# ğŸ”¹ Gemini API ì„¤ì •
genai.configure(api_key=config.GEMINI_API_KEY)

def validate_audio_file(audio_path):
    """ìŒì„± íŒŒì¼ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤."""
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”. ({audio_path})")
    if not audio_path.lower().endswith(config.SUPPORTED_AUDIO_EXTENSIONS):
        raise ValueError(f"âŒ ì˜¤ë¥˜: ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ({audio_path}) ì§€ì› í˜•ì‹: {config.SUPPORTED_AUDIO_EXTENSIONS}")

def transcribe_audio(audio_path):
    """Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    print("ğŸ”¹ [1/4] ìŒì„± íŒŒì¼ ë³€í™˜ ì‹œì‘...")
    try:
        segments, _ = whisper_model.transcribe(audio_path, word_timestamps=True)
        transcript = [f"{segment.start:.2f}-{segment.end:.2f}: {segment.text.strip()}" for segment in segments]
        print("âœ… Whisper ë³€í™˜ ì™„ë£Œ!")
        return transcript
    except Exception as e:
        raise RuntimeError(f"âŒ ì˜¤ë¥˜: Whisper ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def identify_speakers(audio_path):
    """Pyannoteë¥¼ ì‚¬ìš©í•˜ì—¬ í™”ìë¥¼ ì‹ë³„í•©ë‹ˆë‹¤."""
    print("ğŸ”¹ [2/4] í™”ì ë¶„ë¦¬ ìˆ˜í–‰ ì¤‘...")
    try:
        diarization_result = diarization_pipeline(audio_path)
        print("âœ… í™”ì ë¶„ë¦¬ ì™„ë£Œ!")
        return diarization_result
    except Exception as e:
        raise RuntimeError(f"âŒ ì˜¤ë¥˜: í™”ì ë¶„ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def process_speaker_segments(diarization_result, transcript):
    """í™”ì ë¶„ë¦¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°œì–¸ì„ ê·¸ë£¹í™”í•©ë‹ˆë‹¤."""
    speaker_segments = {}
    
    for turn, _, speaker in diarization_result.itertracks(yield_label=True):
        speaker_segments.setdefault(speaker, []).append(turn)
    
    speaker_transcript = []
    for speaker, segments in speaker_segments.items():
        speaker_text = ""
        for segment in segments:
          for transcript_text in transcript:
            start,end,text = re.split(r'[-:]',transcript_text)
            if float(start) <= segment.start and float(end) >= segment.end:
              speaker_text += f'{text} '
              
        if len(speaker_text) > 0:
          speaker_transcript.append(f"{speaker}: {speaker_text.strip()}")

    return speaker_transcript

def summarize_meeting(speaker_transcript):
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ íšŒì˜ë¡ì„ ìš”ì•½í•©ë‹ˆë‹¤."""
    print("ğŸ”¹ [3/4] LLMì„ ì‚¬ìš©í•œ ìš”ì•½ ìƒì„± ì¤‘...")
    try:
        # System ë©”ì‹œì§€ì™€ User ë©”ì‹œì§€ë¥¼ ë¶„ë¦¬í•˜ì—¬ ì „ë‹¬
        prompt = [
            {"role": "system", "parts": [config.SYSTEM_PROMPT]},
            {"role": "user", "parts": [config.USER_PROMPT_TEMPLATE.format(speaker_transcript="\n".join(speaker_transcript))]},
        ]
        
        # LLM íŒŒë¼ë¯¸í„°ë¥¼ configì—ì„œ ê°€ì ¸ì™€ì„œ ì„¤ì • (ë³€ê²½ë¨)
        response = genai.generate_text(
            contents=prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=config.TEMPERATURE,
                top_p=config.TOP_P,
                top_k=config.TOP_K,
                max_output_tokens=config.MAX_OUTPUT_TOKENS
            )
        )
        meeting_summary = response.text
        print("âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ!")
        return meeting_summary
    except Exception as e:
        raise RuntimeError(f"âŒ ì˜¤ë¥˜: ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def save_meeting_log(meeting_summary, speaker_transcript):
    """íšŒì˜ë¡ì„ Markdown íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    print("ğŸ”¹ [4/4] Markdown ì €ì¥ ì¤‘...")
    try:
        filename = f"meeting_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"# ğŸ“ íšŒì˜ë¡\n\n")
            f.write(f"## ğŸ“Œ ìš”ì•½\n{meeting_summary}\n\n")
            f.write(f"## ğŸ”Š ì›ë¬¸\n")
            for line in speaker_transcript:
                f.write(f"- {line}\n")
            f.write(f"\n_(ì´ íšŒì˜ë¡ì€ AIì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤)_\n")

        print(f"âœ… íšŒì˜ë¡ ì €ì¥ ì™„ë£Œ! íŒŒì¼: {filename}")
    except Exception as e:
        raise RuntimeError(f"âŒ ì˜¤ë¥˜: íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def generate_meeting_summary(audio_path):
    """ì „ì²´ íšŒì˜ë¡ ìƒì„± í”„ë¡œì„¸ìŠ¤ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤."""
    try:
        validate_audio_file(audio_path)
        print(f"ğŸ“‚ ì…ë ¥ëœ íŒŒì¼: {audio_path}")

        transcript = transcribe_audio(audio_path)
        diarization_result = identify_speakers(audio_path)
        speaker_transcript = process_speaker_segments(diarization_result, transcript)
        meeting_summary = summarize_meeting(speaker_transcript)
        save_meeting_log(meeting_summary, speaker_transcript)
        
    except (FileNotFoundError, ValueError, RuntimeError) as e:
        print(e)
    finally:
        # ë©”ëª¨ë¦¬ í•´ì œ (í•„ìš”ì— ë”°ë¼ ì¶”ê°€)
        del transcript
        del diarization_result
        del speaker_transcript
        del meeting_summary

# ğŸ”¹ ì‹¤í–‰: ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ íŒŒì¼ ê²½ë¡œ ë°›ê¸°
if __name__ == "__main__":
    audio_path = input("ğŸ¤ ìŒì„± íŒŒì¼ ê²½ë¡œ ì…ë ¥: ").strip()
    generate_meeting_summary(audio_path)
